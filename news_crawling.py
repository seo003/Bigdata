# -*- coding: utf-8 -*-
"""news_beautifulsoup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxKLvMFTJKT2E5jGQx87eQkLR31qdX_6
"""

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime as dt
import json

# 뉴스 데이터 추출 및 저장
def fetch_and_save_news(region, start_year, end_year):
    query = f"{region} 축제"
    json_result = []
    region_file_map = {
        "인천": "incheon",
        "서울": "seoul",
        "경기": "gyeonggi"
    }

    display = 10
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://search.naver.com/'
    }

    for year in range(start_year, end_year + 1):
        for month in range(1, 13):
            cnt = 0
            start_date = f"{year}-{month:02d}-01"
            end_date = f"{year}-{month + 1:02d}-01" if month != 12 else f"{year + 1}-01-01"

            # 날짜 포맷 변경 (YYYY.MM.DD)
            start_date_str = dt.strptime(start_date, "%Y-%m-%d").strftime("%Y.%m.%d")
            end_date_str = dt.strptime(end_date, "%Y-%m-%d").strftime("%Y.%m.%d")

            # 날짜 범위에 맞는 URL 설정
            nso = f"so:r,p:from{start_date.replace('-', '')}to{end_date.replace('-', '')}"
            url = f"https://search.naver.com/search.naver?where=news&query={urllib.parse.quote(query)}&start=1&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds={start_date_str}&de={end_date_str}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso={nso}&is_sug_officeid=0&office_category=0&service_area=0"

            print(f"Requesting news for {region} - {year}-{month:02d}, Page {page}... URL: {url}")
            response = requests.get(url, headers=headers)

            if response.status_code != 200:
                print(f"Failed to fetch data from Naver: {response.status_code}")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = soup.find_all('li', class_='bx')
            print(f"Found {len(news_items)} news items")

            if not news_items:
                break

            for post in news_items:
                title_tag = post.find('a', class_='news_tit')
                if not title_tag:
                    continue
                title = title_tag.text.strip()

                description_tag = post.find('div', class_='news_dsc')
                description = description_tag.text.strip() if description_tag else ''

                date_span = post.find('span', class_='info')
                pub_date = date_span.text.strip() if date_span else None

                if not pub_date:
                    continue

                try:
                    # 날짜 포맷 변경 (YYYY.MM.DD)
                    pub_date_obj = dt.strptime(pub_date, "%Y.%m.%d.")
                except ValueError:
                    # 날짜 포맷이 맞지 않으면 오늘 날짜 사용
                    pub_date_obj = dt.today()

                # 뉴스 내용 출력
                print(f"뉴스 제목: {title}")
                print(f"뉴스 설명: {description}")
                print(f"뉴스 발행일: {pub_date_obj.strftime('%Y-%m-%d')}")
                print("-" * 50)  # 구분선

                # JSON 결과에 추가
                json_result.append({
                    'year': year,
                    'month': month,
                    'title': title,
                    'description': description,
                    'pubDate': pub_date_obj.strftime("%Y-%m-%d %H:%M:%S")
                })

                # 뉴스 20개 이상 크롤링 되면 종료
                cnt += 1
                if cnt >= 20:
                    break

            if cnt >= 20:
                break

            print(f"{year}-{month:02d} 월 크롤링 완료")

    output_file = f"{region_file_map[region]}_{start_year}_{end_year}_news.json"
    print(f"뉴스 파일 저장 경로: {output_file}")
    try:
        with open(output_file, 'w', encoding='utf8') as outfile:
            json.dump(json_result, outfile, indent=4, ensure_ascii=False)
        print(f"{region} 크롤링 완료, 뉴스 저장 파일: {output_file}")
    except Exception as e:
        print(f"파일 저장 중 오류 발생: {e}")

# 메인 함수
if __name__ == '__main__':
    regions = ["인천", "서울", "경기"]
    start_year = 2003
    end_year = 2024

    for region in regions:
        fetch_and_save_news(region, start_year, end_year)